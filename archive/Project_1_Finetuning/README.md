# Fine-Tuning Work â€” ReadMe

ğŸ“„ For context, see the [Fine-Tuning Executive Summary](./EXECUTIVE_SUMMARY.md).

> Before February 2025, I had never fine-tuned a model or configured an AI development environment.  
> Through rapid experimentation (â€œvibe codingâ€) and AI-assisted iteration, I learned to prepare datasets, manage dependencies, and fine-tune large models within two weeks.

---

## Overview
Within **14 days of starting**, I successfully fine-tuned my **second Mistral-7B model** on ~1,200 examples (`vakog_dataset_clean.json`).  
This was my first hands-on technical project â€” a starting point for everything that followed.

After this milestone, I built the **deterministic calendar engine** (Marâ€“Jun 2025) to understand rule-based system design.  
That project later evolved into the **Bitcoin movement predictor** (Julâ€“Aug 2025), which became the technical bridge to my current ENA.ai recursion model.

---

## Results
- **First run:** Model self-prompted (asked and answered its own questions).  
- **Second run (Feb 25):** Fixed self-prompting behavior, achieved correct sensory classification.  
- Dataset size limited clarity, but fine-tuning pipeline worked end-to-end.  
- Demonstrated the ability to independently fine-tune and iterate quickly.

---

## Demo
This fine-tuned model classifies and explains **sensory modalities (VAKOG: Visual, Auditory, Kinesthetic, Olfactory, Gustatory)**.

âš ï¸ *Note:* Outputs sometimes blend modalities due to limited data size â€” but the influence of fine-tuning is clearly visible compared to base model behavior.

**Example prompts**
- â€œWhat modality is *Her eyes sparkled with mischief*?â€ â†’ *Visual*  
- â€œWhy is *Thatâ€™s music to my ears* auditory?â€ â†’ *Auditory*  
- â€œDescribe a storm using the Visual modality.â€  

---

## Reflection
| Date | Milestone |
|------|------------|
| **Feb 9, 2025** | Computer arrived |
| **Feb 25, 2025** | Second fine-tuned model trained (~1,200 examples) |
| **Marâ€“Jun 2025** | Built and completed deterministic calendar engine |
| **Julâ€“Aug 2025** | Extended to Bitcoin price predictor (temporal system) |
| **Oct 2025 â†’** | Built working recursion prototype in 19 days |

Each project built new layers of understanding â€” from empirical fine-tuning â†’ deterministic logic â†’ emotional recursion.

---

## Next Steps
- Expand dataset beyond **5,000 balanced examples** to isolate each modality.  
- Add evaluation prompts for accuracy benchmarking.  
- Apply structured methodology from deterministic systems to future fine-tuning experiments.  

---

*Document: README.md â€” Project_1_Finetuning (Archive)*
